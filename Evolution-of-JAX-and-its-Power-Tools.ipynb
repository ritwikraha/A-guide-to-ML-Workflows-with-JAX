{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/A-guide-to-ML-Workflows-with-JAX/blob/main/Evolution-of-JAX-and-its-Power-Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolution of JAX & its Power Tools\n",
        "\n",
        "So as we discussed, I will be going through the evolution of JAX and how the JAX was conceived and then Aritra will be going through some of the power tools of JAX, to make way for Soumik to finally walk you through a training loop in JAX.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ynbsmCXFPxN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Setup"
      ],
      "metadata": {
        "id": "cnUkZ8W-OAGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCJRV7q4VvWu"
      },
      "outputs": [],
      "source": [
        "# Install autograd\n",
        "!pip install --quiet autograd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary packages\n",
        "from autograd import numpy as anp\n",
        "from autograd import grad\n",
        "from autograd import elementwise_grad as egrad"
      ],
      "metadata": {
        "id": "BjgZAFZ5HOnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The ideation of JAX\n",
        "\n",
        "So obviously JAX is a whole new library and in this introductory lesson we will be going through the chapters shown here, to get a sense of what JAX is and where it came from. "
      ],
      "metadata": {
        "id": "ETCJtwELNVkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Summary of Part 1](https://imgur.com/QCF888w.png)"
      ],
      "metadata": {
        "id": "QviJNvaZMv6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![What is JAX](https://imgur.com/UvKZmPz.png)"
      ],
      "metadata": {
        "id": "Rp9mZjcYNFfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX is the combination of `autograd` and `XLA`.\n"
      ],
      "metadata": {
        "id": "tvOb-Kk5RXgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![JAX equation](https://imgur.com/Au7ExM9.png)"
      ],
      "metadata": {
        "id": "uik3SQM2NIaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay! That is great. We just uncovered a new term and came across two new terms. To understand the essence of what JAX really is, we would need to take a look at autograd and XLA individually.\n"
      ],
      "metadata": {
        "id": "9V_LBxkMRbvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Jargons too many of them](https://imgur.com/nvXtpxr.png)"
      ],
      "metadata": {
        "id": "TFRNT8cFNNuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Various ways of doing differentiation\n",
        "\n",
        "\n",
        "![Various types of differentiation](https://imgur.com/rf0mrnZ.png)\n",
        "\n",
        "And the best way to talk about autograd is to talk about gradients.\n",
        "\n",
        "Gradients run the deep learning world, quite literally. We back propagate the gradients through our DL models which powers them to learn from their mistakes.\n",
        "\n",
        "To compute these gradients we have 4 options.\n",
        "\n",
        "\n",
        "*   Manual: We use our calculus knowledge and derive the derivatives by hand. The problem with this approach is that it is manual. It would take a lot of time for a Deep Learning researcher to derive the model's derivatives by hand.\n",
        "*   Symbolic: We can obtain the derivatives via symbols and a program that can mimic the manual process. The problem with this approach is termed expression swell. Here the derivatives of a particular expression are exponentially \n",
        "longer (think chain rule) than the expression itself. This becomes quite difficult to track.\n",
        "*   Numeric: Here, we use the finite differences method to derive the derivatives. \n",
        "*   Automatic: The star â­ï¸ of the show.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mvuHf65ZNnpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Autograd](https://imgur.com/GNPL1Ym.png)\n",
        "\n",
        "\n",
        "Automatic differentiation (autodiff) is the type of differentiation we all love and use when training our deep neural networks. `autograd` is a python package that performs automatic differentiation on native python and NumPy code. The code base is fairly simple.\n"
      ],
      "metadata": {
        "id": "mHKz0k0PNtM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two points to note here:\n",
        "*  There is a light wrapper `autograd.numpy` around the native NumPy codebase. This allows users to use NumPy-like semantics, harnessing the power of automatic differentiation.\n",
        "*  `autograd.grad` and `autograd.elementwise_grad` help with the actual automatic differentiation.\n"
      ],
      "metadata": {
        "id": "HXLSo1T8UDLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `autograd`\n",
        "\n",
        "$$f(x)=x^2$$"
      ],
      "metadata": {
        "id": "p6tAE5wSINcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_square(value):\n",
        "    # Return the square of the input\n",
        "    return value**2\n",
        "\n",
        "# Build a scalar input and pass it to the\n",
        "# square function\n",
        "value = 4.0\n",
        "value_squared = get_square(value)\n",
        "print(f\"value => {value}\\nvalue**2 => {value_squared}\")"
      ],
      "metadata": {
        "id": "FcgxCc8gHcfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a132ba12-8694-4ac2-a22f-4858615cdde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value => 4.0\n",
            "value**2 => 16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f'(x)=2x$$"
      ],
      "metadata": {
        "id": "Z33jafzewuBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the derivative of the square function\n",
        "grad_func = grad(get_square)\n",
        "\n",
        "point = 1.0\n",
        "# Retrieve the gradient of the function at a particular point\n",
        "print(f\"Gradient of square func at {point} => {grad_func(1.0)}\")"
      ],
      "metadata": {
        "id": "vtiiQxcZHefK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e875bbac-31e8-475e-df4d-45c1076400b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of square func at 1.0 => 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization in `autograd`?"
      ],
      "metadata": {
        "id": "DWEkSiKrxaH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's pass a vector to the square function\n",
        "vector = anp.arange(1, 10, dtype=anp.float32)\n",
        "out_vector = get_square(vector)\n",
        "\n",
        "# Iterate over the vector and its output\n",
        "for v, o in zip(vector, out_vector):\n",
        "    print(f\"Value at point {v} => {o}\")"
      ],
      "metadata": {
        "id": "oMxWAS1_Hh6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c1f36e-d41c-4ca2-d721-77466b4d7abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value at point 1.0 => 1.0\n",
            "Value at point 2.0 => 4.0\n",
            "Value at point 3.0 => 9.0\n",
            "Value at point 4.0 => 16.0\n",
            "Value at point 5.0 => 25.0\n",
            "Value at point 6.0 => 36.0\n",
            "Value at point 7.0 => 49.0\n",
            "Value at point 8.0 => 64.0\n",
            "Value at point 9.0 => 81.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's try the grad function with vector inputs\n",
        "try:\n",
        "    out_vector = grad_func(vector)\n",
        "except Exception as ex:\n",
        "    print(f\"Type of exception => {type(ex).__name__}\")\n",
        "    print(f\"Excpetion => {ex}\")"
      ],
      "metadata": {
        "id": "nMcXp5iIHkM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18722fa-af44-4262-8953-eec2f3c09ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of exception => TypeError\n",
            "Excpetion => Grad only applies to real scalar-output functions. Try jacobian, elementwise_grad or holomorphic_grad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using element wise gradient\n",
        "egrad_func = egrad(get_square)\n",
        "\n",
        "try:\n",
        "    out_vector = egrad_func(vector)\n",
        "    for v, o in zip(vector, out_vector):\n",
        "        print(f\"Grad at point {v} => {o}\")\n",
        "except Exception as ex:\n",
        "    print(f\"Type of exception => {type(ex).__name__}\")\n",
        "    print(f\"Excpetion => {ex}\")"
      ],
      "metadata": {
        "id": "_fxOGp7iHoq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "662f17d3-de56-4700-8164-11fa66e6f417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad at point 1.0 => 2.0\n",
            "Grad at point 2.0 => 4.0\n",
            "Grad at point 3.0 => 6.0\n",
            "Grad at point 4.0 => 8.0\n",
            "Grad at point 5.0 => 10.0\n",
            "Grad at point 6.0 => 12.0\n",
            "Grad at point 7.0 => 14.0\n",
            "Grad at point 8.0 => 16.0\n",
            "Grad at point 9.0 => 18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What about the other parent?\n",
        "\n",
        "It is safe to say that the fields of Deep Learning (DL) and Machine Learning (ML) consist of an enormous amount of Linear Algebra. All computations from start to finish are mostly Linear Algebra. \n",
        "\n",
        "\n",
        "What if we told you there is a compiler in town that can make Linear Algebra operations more efficient? \n",
        "\n",
        "\n",
        "Enters XLA: XLA stands for Accelerated Linear Algebra. It is a domain-specific compiler that accelerates linear algebra operations. \n",
        "\n",
        "The compiled operations are device agnostic. It runs on the CPU, GPU, and TPU with no code change.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qo850YdZUavK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![XLA](https://imgur.com/08C1Lqt.png)"
      ],
      "metadata": {
        "id": "wnpTYp6OOBP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me ask you the question again.\n",
        "\n",
        "What is JAX? Having a fair amount of knowledge about XLA and Autograd should help you with a holistic overview of JAX. \n",
        "\n",
        "It should also make you more at ease for the things that are about to come. \n"
      ],
      "metadata": {
        "id": "7GZxwaXKx7o_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![What is JAX again?](https://imgur.com/o4FwAIK.png)"
      ],
      "metadata": {
        "id": "EDDGuXc7OFQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start multiplying matrices and backpropagating on them, let us take a moment to understand the various components of  JAX. While starting with a library, knowing its basic API design is always a good practice. \n",
        "\n",
        "\n",
        "The API design of JAX is done in a way where we have the high-level abstraction of `jax.numpy` and the low-level abstraction of `jax.lax`.\n",
        "\n",
        "\n",
        "Where `jax.numpy` is similar to the original NumPy package, `jax.lax` is a wrapper around Google's XLA compiler.\n",
        "\n",
        "\n",
        "Note: Did you notice that lax is an anagram of xla? ðŸ¤¯\n",
        "\n",
        "\n",
        "If you head over to the official documentation of JAX API, you will see several sub-packages and sub-topics with their APIs listed. These would be:\n",
        "\n",
        "\n",
        "\n",
        "*   Just-in-time compilation(jit)\n",
        "*   Automatic differentiation(grad)\n",
        "*   Vectorization(vmap)\n",
        "*   Parallelization(pmap)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "And if this sounds alien, don't worry Aritra will be making this a cake walk in a second.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SXyPGY17VEqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Most used APIs](https://imgur.com/Kr4YrZF.png)"
      ],
      "metadata": {
        "id": "uj-jmbnxOKd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import make_jaxpr\n",
        "from jax import grad, jit, vmap, pmap, make_jaxpr"
      ],
      "metadata": {
        "id": "vmhYMRsVw4Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving in, we must note that JAX is not a Deep Learning (DL) framework. Instead, it is a numerical computation library. It is just that DL falls into the numerical computation paradigm. \n",
        "\n",
        "\n",
        "For the ease of numerical computation, it has a NumPy API that mirrors the API of yet another very powerful numerical computation library (yes, you guessed it, NumPy ðŸ˜).\n",
        "\n",
        "\n",
        "The thing that makes JAX stand out is its wrapper for the XLA compiler, `jax.lax`. The `jax.numpy` wrapper is basic XLA code with the `jax.lax` API. This makes JAX code not only device agnostic but also jit compilable.\n",
        "\n",
        "\n",
        "Being device agnostic means that the same code can be run on different hardware (CPUs, GPUs, and TPUs). With the JIT compilation, the same code can run much faster and more efficiently. This is why JAX is referred to as NumPy on steroids. Soumik will be going through this in details.\n",
        "\n"
      ],
      "metadata": {
        "id": "CJ1ByzwFV44H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `jax.numpy`"
      ],
      "metadata": {
        "id": "TXqQxd-vIQuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an array of 0 to 9 with the `jax.numpy` API\n",
        "array = jnp.arange(0, 10, dtype=jnp.int32)\n",
        "\n",
        "print(f\"array => {array}\")\n",
        "print(type(array))"
      ],
      "metadata": {
        "id": "mqllQBndHtD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1057f62-3a73-457e-ea5c-5b5dd3445989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array => [0 1 2 3 4 5 6 7 8 9]\n",
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SET AT\n",
        "jax_array = jnp.arange(1, 10, dtype=jax.numpy.int64)\n",
        "numpy_array = np.arange(1, 10).astype(np.int64)\n",
        "\n",
        "try:\n",
        "    numpy_array[2] = 2\n",
        "except Exception as ex:\n",
        "    print(f\"Type of exception => {type(ex).__name__}\")\n",
        "    print(f\"Excpetion => {ex}\")\n",
        "try:\n",
        "    jax_array[2] = 2\n",
        "except Exception as ex:\n",
        "    print(f\"Type of exception => {type(ex).__name__}\")\n",
        "    print(f\"Excpetion => {ex}\")"
      ],
      "metadata": {
        "id": "w71MGRsvHzcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf288d6-efad-49c5-8898-3baac4785fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of exception => TypeError\n",
            "Excpetion => '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-3bfad3a21464>:2: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in arange is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  jax_array = jnp.arange(1, 10, dtype=jax.numpy.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SET AT\n",
        "try:\n",
        "    mutated_jax_array = jax_array.at[2].set(200)\n",
        "except Exception as ex:\n",
        "    print(f\"Type of exception => {type(ex).__name__}\")\n",
        "    print(f\"Excpetion => {ex}\")\n",
        "print(f\"Original Array => {jax_array}\")\n",
        "print(f\"Mutated Array => {mutated_jax_array}\")"
      ],
      "metadata": {
        "id": "h5GKca8-H1vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f2ead7-1b83-45f6-aa49-456f58d28257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Array => [1 2 3 4 5 6 7 8 9]\n",
            "Mutated Array => [  1   2 200   4   5   6   7   8   9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INDEX OUT OF BOUNDS\n",
        "try:\n",
        "    print(\"Indexing 1000th position of a NumPy array...\")\n",
        "    print(numpy_array[1000])\n",
        "except Exception as ex:\n",
        "    print(type(ex).__name__)\n",
        "    print(ex)"
      ],
      "metadata": {
        "id": "Nuh288JiH3W4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e05cd2-93ba-457b-8a4d-11b0078158f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing 1000th position of a NumPy array...\n",
            "IndexError\n",
            "index 1000 is out of bounds for axis 0 with size 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INDEX OUT OF BOUNDS\n",
        "try:\n",
        "    print(\"Indexing 1000th position of a JAX array...\")\n",
        "    print(jax_array[1000])\n",
        "except Exception as ex:\n",
        "    print(type(ex).__name__)\n",
        "    print(ex)"
      ],
      "metadata": {
        "id": "HOxZ0UtAH4E7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f31bfe-3923-4656-ca8c-2e55655462ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing 1000th position of a JAX array...\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here?\n",
        "In JAX, the indexing is capped. This is a little caveat that we need to take care of so that we do not see our code fail silently.\n",
        "\n",
        "It is being discussed in an [issue](https://github.com/google/jax/issues/9839) (9839) to fix this by returning NaN as a default.\n",
        "\n",
        "Unfortunately the issue is still open.\n",
        "\n"
      ],
      "metadata": {
        "id": "DgcOiY43XGn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![What is next?](https://i.imgur.com/pppIMk1.png)"
      ],
      "metadata": {
        "id": "7V9o_5-bOUJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The power tools of JAX"
      ],
      "metadata": {
        "id": "KguHwF8CMyyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![What we cover?](https://imgur.com/Cvh33Xj.png)"
      ],
      "metadata": {
        "id": "_He5yiPjOw4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Important Functional Transformations](https://imgur.com/KEMRsgp.png)\n",
        "\n",
        "![All of the important functional transformations](https://imgur.com/TjspCdA.png)"
      ],
      "metadata": {
        "id": "dcw6E_G0Oqn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![What are functional transformations](https://imgur.com/WDJWWHu.png)"
      ],
      "metadata": {
        "id": "WuORRqngO7Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Definition](https://imgur.com/KzNskIu.png)"
      ],
      "metadata": {
        "id": "7jFqPbw_O7Tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Definition](https://imgur.com/rxQkhIH.png)"
      ],
      "metadata": {
        "id": "xxtYaYT1O7RR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Pure Functions](https://imgur.com/Nf1pI48.png)"
      ],
      "metadata": {
        "id": "kHmneSQ1O7Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Definition](https://imgur.com/Mo1ygwF.png)"
      ],
      "metadata": {
        "id": "WOmUiQPmO7L-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Checklist](https://imgur.com/rWdu8SZ.png)"
      ],
      "metadata": {
        "id": "Dc3QkmX7O7JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StateFul and StateLess"
      ],
      "metadata": {
        "id": "7vv5RFISuymF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StateFul:\n",
        "    def __init__(self):\n",
        "        self.state = 0\n",
        "    \n",
        "    def change_state(self):\n",
        "        self.state = self.state + 1\n",
        "        output = self.state ** 2\n",
        "        return output\n",
        "\n",
        "stateful = StateFul()\n",
        "print(f\"Initial state => {stateful.state}\")\n",
        "output = stateful.change_state()\n",
        "print(f\"Output => {output}\")\n",
        "print(f\"Changed state => {stateful.state}\")"
      ],
      "metadata": {
        "id": "zcYMxJ25E0jF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e153b6-7879-4b87-b323-281249309f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state => 0\n",
            "Output => 1\n",
            "Changed state => 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StateLess:    \n",
        "    def change_state(self, state):\n",
        "        changed_state = state + 1\n",
        "        output = changed_state ** 2\n",
        "        return output, changed_state\n",
        "\n",
        "stateless = StateLess()\n",
        "initial_state = 0\n",
        "print(f\"Initial state => {initial_state}\")\n",
        "output, changed_state = stateless.change_state(state=initial_state)\n",
        "print(f\"Output => {output}\")\n",
        "print(f\"Changed state => {changed_state}\")"
      ],
      "metadata": {
        "id": "eBz_ggK-kNLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a696b160-b888-4970-d49c-b35d3cb7aeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state => 0\n",
            "Output => 1\n",
            "Changed state => 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple, Any"
      ],
      "metadata": {
        "id": "DdvxWhMMy351"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PureState(NamedTuple):\n",
        "    state: Any\n",
        "\n",
        "    def update_state(self, new_state):\n",
        "        return PureState(new_state)\n",
        "\n",
        "p1 = PureState(1)\n",
        "p2 = p1.update_state(2)\n",
        "\n",
        "print(p1)\n",
        "print(p2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erm2u8ktyfjI",
        "outputId": "43aebcd9-0f17-4b7b-d018-ccaf01f6c116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PureState(state=1)\n",
            "PureState(state=2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jaxpr\n",
        "\n",
        "![jaxpr](https://imgur.com/VQuRKVP.png)"
      ],
      "metadata": {
        "id": "W93_FEt7u2bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Definition](https://imgur.com/xbmzea4.png)"
      ],
      "metadata": {
        "id": "Pm04R2aTPprq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![jaxpr](https://imgur.com/IiynmyJ.png)"
      ],
      "metadata": {
        "id": "E91IiORcPtP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![jaxpr illustrated](https://imgur.com/KDO7A2i.png)"
      ],
      "metadata": {
        "id": "ZvO6W2LOPxj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_function(arg1, arg2, arg3):\n",
        "    temp = arg1 + arg2\n",
        "    temp = temp * arg3\n",
        "    return temp / 3.0\n",
        "\n",
        "make_jaxpr(demo_function)(1.0, 1.0, 1.0)"
      ],
      "metadata": {
        "id": "D0XxajsDspoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8745afa0-d351-441b-c3de-e0406d3791ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[] b:f32[] c:f32[]. let\n",
              "    d:f32[] = add a b\n",
              "    e:f32[] = mul d c\n",
              "    f:f32[] = div e 3.0\n",
              "  in (f,) }"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![illustrated function](https://imgur.com/nOA1NSq.png)"
      ],
      "metadata": {
        "id": "SUajhnoNP3e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `grad`\n",
        "\n",
        "![jax grad](https://imgur.com/LP5SppJ.png)"
      ],
      "metadata": {
        "id": "qlF9ocU8u_wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equation(x):\n",
        "    return 4*x**3 + 3*x**2 + 2*x + 1\n",
        "\n",
        "make_jaxpr(equation)(2.0)"
      ],
      "metadata": {
        "id": "brgeCbpGQGFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b33b8a-9aa7-42f2-b4ec-a60314eb7b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[]. let\n",
              "    b:f32[] = integer_pow[y=3] a\n",
              "    c:f32[] = mul 4.0 b\n",
              "    d:f32[] = integer_pow[y=2] a\n",
              "    e:f32[] = mul 3.0 d\n",
              "    f:f32[] = add c e\n",
              "    g:f32[] = mul 2.0 a\n",
              "    h:f32[] = add f g\n",
              "    i:f32[] = add h 1.0\n",
              "  in (i,) }"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![jaxpr of the function](https://imgur.com/odXjT3D.png)"
      ],
      "metadata": {
        "id": "xryL_K66QQh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "equation_first_der = grad(equation)\n",
        "make_jaxpr(equation_first_der)(2.0)"
      ],
      "metadata": {
        "id": "W21RXA9Ws_KY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701ba0f4-5315-4df2-9b48-15a938c92e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[]. let\n",
              "    b:f32[] = integer_pow[y=3] a\n",
              "    c:f32[] = integer_pow[y=2] a\n",
              "    d:f32[] = mul 3.0 c\n",
              "    e:f32[] = mul 4.0 b\n",
              "    f:f32[] = integer_pow[y=2] a\n",
              "    g:f32[] = integer_pow[y=1] a\n",
              "    h:f32[] = mul 2.0 g\n",
              "    i:f32[] = mul 3.0 f\n",
              "    j:f32[] = add e i\n",
              "    k:f32[] = mul 2.0 a\n",
              "    l:f32[] = add j k\n",
              "    _:f32[] = add l 1.0\n",
              "    m:f32[] = mul 2.0 1.0\n",
              "    n:f32[] = mul 3.0 1.0\n",
              "    o:f32[] = mul n h\n",
              "    p:f32[] = add_any m o\n",
              "    q:f32[] = mul 4.0 1.0\n",
              "    r:f32[] = mul q d\n",
              "    s:f32[] = add_any p r\n",
              "  in (s,) }"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![jaxpr of the derivative of the function defined](https://imgur.com/TZBlWUF.png)"
      ],
      "metadata": {
        "id": "4OJi_I-pQVnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "equation_first_der(2.0)"
      ],
      "metadata": {
        "id": "J7umk-sgtCwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464e3547-8b0b-4d1c-dc6e-d6adcfb0f27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(62., dtype=float32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "equation_second_der = grad(equation_first_der)\n",
        "equation_second_der(2.0)"
      ],
      "metadata": {
        "id": "6xKpRDMxtHwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdae1615-f249-498c-8e53-8eed813a560e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(54., dtype=float32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "equation_third_der = grad(equation_second_der)\n",
        "equation_third_der(2.0)"
      ],
      "metadata": {
        "id": "CQRa4E6ttX5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98c8251-b7c4-4ed3-ede0-1ab1271b4c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(24., dtype=float32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `jit`\n",
        "\n",
        "![jit](https://imgur.com/QNB8mMN.png)"
      ],
      "metadata": {
        "id": "P40prWp8vDyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![steps for jit](https://imgur.com/eoRqRRM.png)"
      ],
      "metadata": {
        "id": "kGX9LcOLQieg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_mul(a, b):\n",
        "    return jnp.matmul(a, b)\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "a = jax.random.normal(key, shape=(1000, 5000))\n",
        "b = jax.random.normal(key, shape=(5000, 1000))\n",
        "\n",
        "make_jaxpr(matrix_mul)(a, b)"
      ],
      "metadata": {
        "id": "NK_5gUM9tc_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209f1cc4-226b-4989-fef5-33a389e41d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[1000,5000] b:f32[5000,1000]. let\n",
              "    c:f32[1000,1000] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a b\n",
              "  in (c,) }"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -n5 matrix_mul(a, b).block_until_ready()"
      ],
      "metadata": {
        "id": "q2EoxBYttgLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef4ba3c-9347-4862-b370-dbfe965b02b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "252 ms Â± 86.9 ms per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jit_matrix_mul = jit(matrix_mul)\n",
        "\n",
        "make_jaxpr(jit_matrix_mul)(a, b)"
      ],
      "metadata": {
        "id": "0tB-wcz5tjRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66df02c-af25-4bee-8b23-c328c3accc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[1000,5000] b:f32[5000,1000]. let\n",
              "    c:f32[1000,1000] = pjit[\n",
              "      jaxpr={ lambda ; d:f32[1000,5000] e:f32[5000,1000]. let\n",
              "          f:f32[1000,1000] = dot_general[\n",
              "            dimension_numbers=(([1], [0]), ([], []))\n",
              "          ] d e\n",
              "        in (f,) }\n",
              "      name=matrix_mul\n",
              "    ] a b\n",
              "  in (c,) }"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![xla in the jaxpr](https://imgur.com/7mkyL2n.png)"
      ],
      "metadata": {
        "id": "Z7bQ8nfpQp7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# warmup\n",
        "warmup_results = jit_matrix_mul(a, b)\n",
        "\n",
        "# âš¡ï¸ speed em up!\n",
        "%timeit -n5 jit_matrix_mul(a, b).block_until_ready()"
      ],
      "metadata": {
        "id": "uCyf03O4tmQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3523ae88-f228-455c-80cc-7056f5716db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "183 ms Â± 25.8 ms per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Can we JIT everything?"
      ],
      "metadata": {
        "id": "QPoWc4ai0YTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's break JIT now\n",
        "@jit\n",
        "def f(x):\n",
        "    if x > 0:\n",
        "        return x+1\n",
        "    else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "P6jXHrcKtpGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    f(10)\n",
        "except Exception as ex:\n",
        "    print(f\"Type of exception => {type(ex).__name__}\")\n",
        "    print(f\"Exception => {ex}\")"
      ],
      "metadata": {
        "id": "vH3BkVyVtszt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb759d82-c8d6-4e11-8817-90d6f757582b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of exception => ConcretizationTypeError\n",
            "Exception => Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n",
            "The problem arose with the `bool` function. \n",
            "The error occurred while tracing the function f at <ipython-input-46-e3be4ae0da4e>:2 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n",
            "\n",
            "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def f(x):\n",
        "    return jnp.where(x > 0, x+1, x)"
      ],
      "metadata": {
        "id": "FJppRFHr0chW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    f(10)\n",
        "except Exception as ex:\n",
        "    print(f\"Type of exception => {type(ex).__name__}\")\n",
        "    print(f\"Exception => {ex}\")"
      ],
      "metadata": {
        "id": "SvOTTAxY0iFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `vmap`"
      ],
      "metadata": {
        "id": "tVJJHjP_vU9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([1.0, 4.0, 0.5])\n",
        "b = jnp.arange(5, 10, dtype=jnp.float32)\n",
        "\n",
        "def weighted_mean(a, b):\n",
        "    output = []\n",
        "    for idx in range(1, b.shape[0]-1):\n",
        "        output.append(jnp.mean(a + b[idx-1 : idx+2]))\n",
        "    return jnp.array(output)\n",
        "\n",
        "print(f\"a => {a.shape}\")\n",
        "print(f\"b => {b.shape}\")\n",
        "output = weighted_mean(a, b)\n",
        "print(f\"output => {output.shape}\")"
      ],
      "metadata": {
        "id": "_aRK-Gastz3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81aff175-8568-4656-b9ac-051fd71deeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a => (3,)\n",
            "b => (5,)\n",
            "output => (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's include the batch dim to the inputs\n",
        "batch_size = 8\n",
        "batched_a = jnp.stack([a] * batch_size)\n",
        "batched_b = jnp.stack([b] * batch_size)\n",
        "\n",
        "print(f\"batched_a => {batched_a.shape}\")\n",
        "print(f\"batched_b => {batched_b.shape}\")"
      ],
      "metadata": {
        "id": "H_7uHiott4P3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfbe5d9-ec02-4d5d-ed52-91a5d297394d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batched_a => (8, 3)\n",
            "batched_b => (8, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batched_weighted_mean = vmap(weighted_mean)\n",
        "batched_output = batched_weighted_mean(batched_a, batched_b)\n",
        "\n",
        "print(f\"batched output => {batched_output.shape}\")"
      ],
      "metadata": {
        "id": "Aso_b01wt66F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7314b059-5e7c-4f87-d832-9483bb5c49a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batched output => (8, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `pmap`\n",
        "\n",
        "For this section you would need to go to the `Runtime` of the colab notebook and change runtime to TPU."
      ],
      "metadata": {
        "id": "XQZmy0tfvb36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "egX5beikuIht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import numpy as jnp\n",
        "from jax import pmap\n",
        "from jax import random"
      ],
      "metadata": {
        "id": "07DXf7VvvnI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = random.PRNGKey(42)\n",
        "a = random.normal(key, shape=(3000,5000))\n",
        "b = random.normal(key, shape=(5000,3000))\n",
        "\n",
        "matrix_mul = lambda a, b: jnp.matmul(a, b)\n",
        "matrix_mul(a, b).shape"
      ],
      "metadata": {
        "id": "2erD25XpvwqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count()\n",
        "a = random.normal(key, shape=(n_devices, 3000, 5000))\n",
        "b = random.normal(key, shape=(n_devices, 5000, 3000))\n",
        "\n",
        "parallel_matrix_mul = pmap(matrix_mul)\n",
        "parallel_matrix_mul(a, b).shape"
      ],
      "metadata": {
        "id": "YMlukvGqvzlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ² Randomness"
      ],
      "metadata": {
        "id": "or-NyW487g3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# random number generation using numpy\n",
        "\n",
        "np.random.seed(42)\n",
        "rn1 = np.random.normal()\n",
        "rn2 = np.random.normal()\n",
        "print(f\"NumPy Random Number Generation: {rn1: .2f} {rn2: .2f}\")\n"
      ],
      "metadata": {
        "id": "YHLB-yi_UzPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(65)\n",
        "\n",
        "print(key)\n",
        "\n",
        "jrn1 = random.normal(key)\n",
        "jrn2 = random.normal(key)\n",
        "\n",
        "print(f\"JAX Random Number Generation: {jrn1: .2f} {jrn2: .2f}\")"
      ],
      "metadata": {
        "id": "9Xsua1o6U2oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"JAX original key\", key)\n",
        "mod_key, subkey = random.split(key)\n",
        "\n",
        "print(\"JAX modified key\", mod_key)\n",
        "print(\"JAX sub key\", subkey)"
      ],
      "metadata": {
        "id": "-rzn_LBmU7xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![What is next?](https://i.imgur.com/pppIMk1.png)"
      ],
      "metadata": {
        "id": "oOHdRLpTcKD7"
      }
    }
  ]
}