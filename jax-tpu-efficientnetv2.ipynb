{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom typing import Any\nfrom functools import partial\n\nimport jax\nfrom jax import lax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\nimport flax\nfrom flax import core\nimport flax.linen as nn\nfrom flax.training import common_utils\n\nimport optax\nimport msgpack\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport math\nimport wandb\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom kaggle_datasets import KaggleDatasets\nfrom kaggle_secrets import UserSecretsClient\n\n\nprint(\"Number of devices:\", jax.device_count())","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:39.313138Z","iopub.execute_input":"2023-03-14T17:55:39.313673Z","iopub.status.idle":"2023-03-14T17:55:47.287678Z","shell.execute_reply.started":"2023-03-14T17:55:39.313638Z","shell.execute_reply":"2023-03-14T17:55:47.286191Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"[percpu.cc : 552] RAW: rseq syscall failed with errno 1\n/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"Number of devices: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"wandb_api_key\")\nwandb.init(project=\"transfer-learning-jax\", entity=\"ml-colabs\", job_type=\"train\")\n\nconfig = wandb.config\n\nconfig.seed = 0\n\nconfig.classes = [\n    'pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',         'wild geranium',      'tiger lily',\n    'moon orchid',      'bird of paradise',          'monkshood',        'globe thistle',     'snapdragon',         \"colt's foot\",\n    'king protea',      'spear thistle',             'yellow iris',      'globe-flower',      'purple coneflower',  'peruvian lily',\n    'balloon flower',   'giant white arum lily',     'fire lily',        'pincushion flower', 'fritillary',         'red ginger',\n    'grape hyacinth',   'corn poppy',                'prince of wales feathers',              'stemless gentian',   'artichoke',\n    'sweet william',    'carnation',                 'garden phlox',     'love in the mist',  'cosmos',             'alpine sea holly',\n    'ruby-lipped cattleya', 'cape flower',           'great masterwort', 'siam tulip',        'lenten rose',        'barberton daisy',\n    'daffodil',         'sword lily',                'poinsettia',       'bolero deep blue',  'wallflower',         'marigold',\n    'buttercup',        'daisy',                     'common dandelion', 'petunia',           'wild pansy',         'primula',\n    'sunflower',        'lilac hibiscus',            'bishop of llandaff',                    'gaura',              'geranium',\n    'orange dahlia',    'pink-yellow dahlia',        'cautleya spicata', 'japanese anemone',  'black-eyed susan',   'silverbush',\n    'californian poppy',                             'osteospermum',     'spring crocus',     'iris',               'windflower',\n    'tree poppy',       'gazania',                   'azalea',           'water lily',        'rose',               'thorn apple',\n    'morning glory',    'passion flower',            'lotus',            'toad lily',         'anthurium',          'frangipani',\n    'clematis',         'hibiscus',                  'columbine',        'desert-rose',       'tree mallow',        'magnolia',\n    'cyclamen ',        'watercress',                'canna lily',       'hippeastrum ',      'bee balm',           'pink quill',\n    'foxglove',         'bougainvillea',             'camellia',         'mallow',            'mexican petunia',    'bromelia',\n    'blanket flower',   'trumpet creeper',           'blackberry lily',  'common tulip',      'wild rose'\n]\nconfig.num_classes = 2**(math.ceil(math.log2(len(config.classes))))\n\nconfig.image_size = 512\nconfig.data_path = os.path.join(\n    \"/kaggle/input/tpu-getting-started\", f'tfrecords-jpeg-{config.image_size}x{config.image_size}'\n)\n\nconfig.efficientnet_v2_path = \"/kaggle/input/efficientnetv2-jax/efficientnetv2-m.msgpack\"\nconfig.efficientnet_stem_size = 24\nconfig.efficientnet_arch_configs = [\n    [1,  24,  3, 1, 0],\n    [4,  48,  5, 2, 0],\n    [4,  80,  5, 2, 0],\n    [4, 160,  7, 2, 1],\n    [6, 176, 14, 1, 1],\n    [6, 304, 18, 2, 1],\n    [6, 512,  5, 1, 1],\n]\n\nconfig.batch_size = 8\nconfig.num_devices = jax.device_count()\nconfig.learning_rate = 7e-5 * config.num_devices\nconfig.weight_decay = 1e-4\nconfig.epochs = 5","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:47.290177Z","iopub.execute_input":"2023-03-14T17:55:47.290576Z","iopub.status.idle":"2023-03-14T17:55:49.620892Z","shell.execute_reply.started":"2023-03-14T17:55:47.290538Z","shell.execute_reply":"2023-03-14T17:55:49.619687Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgeekyrakshit\u001b[0m (\u001b[33mml-colabs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230314_175548-edbx0tly</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/ml-colabs/transfer-learning-jax/runs/edbx0tly\" target=\"_blank\">pear-cobbler-2</a></strong> to <a href=\"https://wandb.ai/ml-colabs/transfer-learning-jax\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}}]},{"cell_type":"code","source":"TRAIN_FILES = tf.io.gfile.glob(os.path.join(config.data_path, 'train', '*.tfrec'))\nVAL_FILES = tf.io.gfile.glob(os.path.join(config.data_path, 'val', '*.tfrec'))\nTEST_FILES = tf.io.gfile.glob(os.path.join(config.data_path, 'test', '*.tfrec'))\n\nAUTOTUNE = tf.data.AUTOTUNE\n\nprint(\"Number of Train files:\", len(TRAIN_FILES))\nprint(\"Number of Validation files:\", len(VAL_FILES))\nprint(\"Number of Test files:\", len(TEST_FILES))","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:49.622414Z","iopub.execute_input":"2023-03-14T17:55:49.622778Z","iopub.status.idle":"2023-03-14T17:55:49.658616Z","shell.execute_reply.started":"2023-03-14T17:55:49.622740Z","shell.execute_reply":"2023-03-14T17:55:49.657602Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of Train files: 16\nNumber of Validation files: 16\nNumber of Test files: 16\n","output_type":"stream"}]},{"cell_type":"code","source":"def decode_image(image_data):\n    \"\"\"Given a `tf.string`, returns a legible `tf.tensor`\"\"\"\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.reshape(image, [config.image_size, config.image_size, 3]) \n    return image\n\n\ndef read_labeled_tfrecord(example):\n    \"\"\"\n    Parses general type example.\n    Returns data sample (tuple of (image : ), (label : ))\n    \"\"\"\n    labeled_format = {\n        # tf.string means bytestring\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        # shape [] means single element\n        \"class\": tf.io.FixedLenFeature([], tf.int64),\n    }\n    parsed_example = tf.io.parse_single_example(example, labeled_format)\n    image = decode_image(parsed_example['image'])\n    label = tf.cast(parsed_example['class'], tf.int32)\n    return {'image': image, 'label': label}\n\n\ndef read_unlabeled_tfrecord(example):\n    \"\"\"\n    Parses general type example, useful for parsing test tfrecord files.\n    Returns data sample (tuple of (image : ), (label : ))\n    \"\"\"\n    unlabeled_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"id\": tf.io.FixedLenFeature([], tf.string),\n    }\n    parsed_example = tf.io.parse_single_example(example, unlabeled_format)\n    image = decode_image(parsed_example['image'])\n    idnum = parsed_example['id']\n    return {'image': image, 'id': idnum}\n\n\ndef normalize(sample):\n    \"\"\"Given a parsed tfrecord sample, returns the same sample with a normalized image.\"\"\"\n    sample['image'] = tf.cast(sample['image'], tf.float32) / 128. - 1.\n    return sample\n\n\ndef to_jax(sample):\n    \"\"\"Given a parsed tfrecord sample, converts it to JAX-pipeline-compatible format.\"\"\"\n    sample['image'] = jnp.array(sample['image'], dtype=jnp.bfloat16)\n    sample['label'] = jnp.array(sample['label'], dtype=jnp.int16)\n    # Convert labels to one_hot\n    sample['label'] = jax.nn.one_hot(\n        sample['label'], config.num_classes, dtype=jnp.int16, axis=-1\n    )\n    return common_utils.shard(sample)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:49.659962Z","iopub.execute_input":"2023-03-14T17:55:49.660286Z","iopub.status.idle":"2023-03-14T17:55:49.677567Z","shell.execute_reply.started":"2023-03-14T17:55:49.660232Z","shell.execute_reply":"2023-03-14T17:55:49.676542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def create_dataloader(\n    tfrecord_files,\n    is_labeled: bool,\n    is_ordered: bool,\n    shuffle_buffer_size: int,\n    drop_remainder: bool\n):\n    dataset = tf.data.TFRecordDataset(tfrecord_files, num_parallel_reads=AUTOTUNE)\n    \n    options = tf.data.Options()\n    if not is_ordered:\n        options.experimental_deterministic = False\n    dataset = dataset.with_options(options)\n    \n    \n    if is_labeled:\n        dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTOTUNE)\n    else:\n        dataset = dataset.map(read_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE)\n    \n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(\n        config.batch_size * config.num_devices, drop_remainder=drop_remainder\n    )\n    dataset = dataset.map(normalize)\n    dataset = dataset.prefetch(AUTOTUNE)\n    \n    dataset = tfds.as_numpy(dataset)\n    dataset = map(to_jax, dataset)\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:49.680462Z","iopub.execute_input":"2023-03-14T17:55:49.680759Z","iopub.status.idle":"2023-03-14T17:55:49.708938Z","shell.execute_reply.started":"2023-03-14T17:55:49.680730Z","shell.execute_reply":"2023-03-14T17:55:49.707912Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_loader = create_dataloader(\n    TRAIN_FILES,\n    is_labeled=True,\n    is_ordered=False,\n    shuffle_buffer_size=4 * config.batch_size,\n    drop_remainder=True\n)\n\nval_loader = create_dataloader(\n    VAL_FILES,\n    is_labeled=True,\n    is_ordered=True,\n    shuffle_buffer_size=4 * config.batch_size,\n    drop_remainder=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:49.710163Z","iopub.execute_input":"2023-03-14T17:55:49.710473Z","iopub.status.idle":"2023-03-14T17:55:50.062280Z","shell.execute_reply.started":"2023-03-14T17:55:49.710446Z","shell.execute_reply":"2023-03-14T17:55:50.061072Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Efficientnet-V2","metadata":{}},{"cell_type":"code","source":"conv_init = nn.initializers.variance_scaling(\n    2., mode='fan_out',\n    distribution=\"truncated_normal\",\n    dtype=jnp.bfloat16\n)\ndense_init = nn.initializers.variance_scaling(\n    1./3, mode='fan_out',\n    distribution=\"truncated_normal\",\n    dtype=jnp.bfloat16\n)\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:50.063683Z","iopub.execute_input":"2023-03-14T17:55:50.063973Z","iopub.status.idle":"2023-03-14T17:55:50.073015Z","shell.execute_reply.started":"2023-03-14T17:55:50.063943Z","shell.execute_reply":"2023-03-14T17:55:50.072023Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class SqueezeExcitationLayer(nn.Module):\n    input_channels : int\n    output_channels : int\n    dtype : Any\n    reduction : int = 4\n        \n    @nn.compact \n    def __call__(self, x):\n        features = _make_divisible(self.input_channels // self.reduction, 4)\n        y = nn.Conv(\n            features,\n            kernel_size=(1, 1),\n            param_dtype=self.dtype,\n            dtype=self.dtype\n        )(x)\n        y = jax.nn.silu(y)\n        y = nn.Conv(\n            self.output_channels,\n            kernel_size=(1, 1),\n            param_dtype=self.dtype,\n            dtype=self.dtype\n        )(y)\n        y = jax.nn.sigmoid(y)\n        return x * y","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:50.074381Z","iopub.execute_input":"2023-03-14T17:55:50.075014Z","iopub.status.idle":"2023-03-14T17:55:50.087534Z","shell.execute_reply.started":"2023-03-14T17:55:50.074981Z","shell.execute_reply":"2023-03-14T17:55:50.086500Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    output_channels : int\n    kernel_size : int\n    dtype : Any \n    stride : int = 1\n    groups : int = 1\n    dropout_rate : float = 0.\n    # Whether to do activation\n    use_activation : bool = True\n    apply_skip_connection : bool = False\n        \n    @nn.compact \n    def __call__(self, x):\n        if self.apply_skip_connection:\n            shortcut = x\n        x = nn.Conv(\n            self.output_channels,\n            kernel_size=(self.kernel_size, self.kernel_size), \n            strides=self.stride,\n            feature_group_count=self.groups,\n            use_bias=False,\n            param_dtype=self.dtype,\n            dtype=self.dtype,\n            kernel_init=conv_init\n        )(x)\n        mutable = self.is_mutable_collection('batch_stats')\n        if self.dropout_rate != 0.:\n            x = nn.Dropout(\n                rate=self.dropout_rate, deterministic=not mutable\n            )(x)\n        x = nn.BatchNorm(\n            momentum=.9,\n            use_running_average=not mutable,\n            param_dtype=self.dtype,\n            dtype=self.dtype,\n            axis_name='devices'\n        )(x)\n        if self.use_activation:\n            x = jax.nn.silu(x)\n        if self.apply_skip_connection:\n            return x + shortcut \n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:50.088809Z","iopub.execute_input":"2023-03-14T17:55:50.089121Z","iopub.status.idle":"2023-03-14T17:55:50.111009Z","shell.execute_reply.started":"2023-03-14T17:55:50.089093Z","shell.execute_reply":"2023-03-14T17:55:50.110035Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class DropBlock(nn.Module):\n    dropblock_rate : float = .0\n    @nn.compact \n    def __call__(self, x):\n        mutable = self.is_mutable_collection('batch_stats')\n        if mutable:\n            pred = jax.random.bernoulli(\n                self.make_rng('dropout'), p=self.dropblock_rate\n            )\n            return jax.lax.cond(pred, lambda x : 0., lambda x: 1., 0) * x\n        else:\n            return (1 - self.dropblock_rate) * x","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:50.112013Z","iopub.execute_input":"2023-03-14T17:55:50.112282Z","iopub.status.idle":"2023-03-14T17:55:50.128238Z","shell.execute_reply.started":"2023-03-14T17:55:50.112243Z","shell.execute_reply":"2023-03-14T17:55:50.127195Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class MBConv(nn.Module):\n    input_channels : int\n    output_channels : int \n    stride : int\n    expand_ratio : float \n    use_squeeze_excitation : bool \n    dtype : Any \n    dropout_rate : float = 0. \n    dropblock_rate : float = 0.\n        \n    def setup(self):\n        assert self.stride in [1, 2]\n\n        hidden_dim = round(self.input_channels * self.expand_ratio)\n        self.identity = self.stride == 1 and self.input_channels == self.output_channels\n        if self.dropblock_rate != 0:\n            self.DropBlock = DropBlock(dropblock_rate=self.dropblock_rate)\n        if self.use_squeeze_excitation:\n            self.conv = nn.Sequential([\n                ConvBlock(\n                    output_channels=hidden_dim,\n                    kernel_size=1,\n                    stride=1,\n                    dtype=self.dtype,\n                    dropout_rate=self.dropout_rate\n                ),\n                ConvBlock(\n                    output_channels=hidden_dim,\n                    kernel_size=3,\n                    stride=self.stride,\n                    groups=hidden_dim,\n                    dtype=self.dtype,\n                    dropout_rate=self.dropout_rate\n                ),\n                SqueezeExcitationLayer(\n                    input_channels=self.input_channels,\n                    output_channels=hidden_dim,\n                    dtype=self.dtype\n                ),\n                nn.Conv(\n                    self.output_channels,\n                    kernel_size=(1, 1),\n                    use_bias=False,\n                    param_dtype=self.dtype,\n                    dtype=self.dtype,\n                    kernel_init=conv_init),\n            ])\n        else:\n            self.conv = nn.Sequential([\n                ConvBlock(\n                    output_channels=hidden_dim,\n                    kernel_size=3,\n                    stride=self.stride,\n                    dtype=self.dtype,\n                    dropout_rate=self.dropout_rate\n                ),\n                nn.Conv(\n                    self.output_channels,\n                    kernel_size=(1, 1),\n                    use_bias=False,\n                    param_dtype=self.dtype,\n                    dtype=self.dtype,\n                    kernel_init=conv_init\n                ),\n            ])\n        self.bn = nn.BatchNorm(\n            momentum=.9,\n            param_dtype=self.dtype,\n            dtype=self.dtype,\n            axis_name='devices'\n        )\n\n    # Well this .remat almost is not doing anything\n    @nn.remat\n    def __call__(self, x):\n        mutable = self.is_mutable_collection('batch_stats')\n        if self.identity:\n            if self.dropblock_rate != 0:\n                return x + self.DropBlock(\n                    self.bn(self.conv(x), use_running_average=not mutable)\n                )\n            else:\n                return x + self.bn(self.conv(x), use_running_average=not mutable)\n        else:\n            return self.bn(self.conv(x), use_running_average=not mutable)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:50.129539Z","iopub.execute_input":"2023-03-14T17:55:50.130145Z","iopub.status.idle":"2023-03-14T17:55:50.153141Z","shell.execute_reply.started":"2023-03-14T17:55:50.130110Z","shell.execute_reply":"2023-03-14T17:55:50.152200Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class EfficientNetV2(nn.Module):\n    dtype : Any\n    architecture_configs : list \n    width_mult : float = 1.\n    dropout_rate : float = 0.\n    stem_size : int = 24\n    dropblock_rate : float = .0\n\n    def setup(self):\n        # building first layer\n        input_channel = self.stem_size\n        # This is the conv_stem\n        self.conv_stem = ConvBlock(\n            output_channels=input_channel,\n            kernel_size=3,\n            stride=2,\n            dtype=self.dtype,\n            dropout_rate=self.dropout_rate,\n            use_activation=False\n        )\n        # Main computational part\n        total_layers = sum([n for (t, c, n, s, use_squeeze_excitation) in self.architecture_configs[1:]])\n        layer_count = 1\n        layers = []\n        for j, (t, c, n, s, use_squeeze_excitation) in enumerate(self.architecture_configs):\n            output_channel = _make_divisible(c * self.width_mult, 8)\n            block_layers = []\n            for i in range(n):\n                # The first one should be simply ConvBlock\n                if j == 0:\n                    block_layers.append(ConvBlock(\n                        output_channels=input_channel,\n                        kernel_size=3,\n                        stride=1,\n                        dtype=self.dtype,\n                        dropout_rate=self.dropout_rate,\n                        apply_skip_connection=True\n                    ))\n                else:\n                    layer_count += 1\n                    # Progressive dropblock of MBConv layers\n                    block_layers.append(MBConv(\n                        input_channels=input_channel,\n                        output_channels=output_channel,\n                        stride=s if i == 0 else 1,\n                        expand_ratio=t,\n                        use_squeeze_excitation=use_squeeze_excitation,\n                        dtype=self.dtype,\n                        dropout_rate=self.dropout_rate,\n                        dropblock_rate=self.dropblock_rate * layer_count / (total_layers + 1)\n                    ))\n                input_channel = output_channel\n            layers.append(nn.Sequential(block_layers))\n        self.features = nn.Sequential(layers)\n        # building last several layers\n        self.output_channel = _make_divisible(1792 * self.width_mult, 8) if self.width_mult > 1.0 else 1280\n        self.conv_head = ConvBlock(\n            output_channels=self.output_channel,\n            kernel_size=1,\n            dtype=self.dtype,\n            dropout_rate=self.dropout_rate\n        )\n\n    def __call__(self, x):\n        return self.conv_head(self.features(self.conv_stem(x)))","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:50.154693Z","iopub.execute_input":"2023-03-14T17:55:50.155001Z","iopub.status.idle":"2023-03-14T17:55:50.174388Z","shell.execute_reply.started":"2023-03-14T17:55:50.154973Z","shell.execute_reply":"2023-03-14T17:55:50.173442Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"random_key = jax.random.PRNGKey(config.seed)\nrandom_key, efficientnet_key = jax.random.split(random_key)\n\nbackbone_model = EfficientNetV2(\n    architecture_configs=config.efficientnet_arch_configs,\n    dtype=jnp.bfloat16,\n    dropout_rate=0.0,\n    dropblock_rate=0.0,\n    stem_size=config.efficientnet_stem_size\n)\n\ndummy_input = jnp.ones((1, config.image_size, config.image_size, 3), dtype=jnp.bfloat16)\nbackbone_params = backbone_model.init(\n    {'dropout': efficientnet_key, 'params': efficientnet_key}, dummy_input\n)\n\nwith open(config.efficientnet_v2_path, \"rb\") as f:\n    byte_data = f.read()\nbackbone_params = flax.serialization.from_bytes(backbone_params, byte_data)\n\nbackbone_params = jax.tree_map(lambda x : x.astype(jnp.bfloat16), backbone_params)\nbackbone_params = flax.core.unfreeze(backbone_params)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:55:50.175672Z","iopub.execute_input":"2023-03-14T17:55:50.175972Z","iopub.status.idle":"2023-03-14T17:56:48.511935Z","shell.execute_reply.started":"2023-03-14T17:55:50.175944Z","shell.execute_reply":"2023-03-14T17:56:48.510799Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class EfficientNetV2ClassificationModel(nn.Module):\n    backbone: Any\n    num_features : int\n    dtype : Any\n    \n    @nn.compact \n    def __call__(self, x):\n        mutable = self.is_mutable_collection('batch_stats')\n        x = self.backbone(x)\n        x = jax.nn.swish(jnp.mean(x, axis=(1, 2)))\n        return nn.Dense(\n            self.num_features,\n            use_bias=False,\n            param_dtype=self.dtype,\n            dtype=self.dtype\n        )(x)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:48.515855Z","iopub.execute_input":"2023-03-14T17:56:48.516182Z","iopub.status.idle":"2023-03-14T17:56:48.525510Z","shell.execute_reply.started":"2023-03-14T17:56:48.516151Z","shell.execute_reply":"2023-03-14T17:56:48.524655Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"random_key, classifier_key_1, classifier_key_2 = jax.random.split(random_key, 3)\n\nclassification_model = EfficientNetV2ClassificationModel(\n    backbone=backbone_model, num_features=config.num_classes, dtype=jnp.bfloat16\n)\nparams = classification_model.init({'params': classifier_key_1, 'dropout': classifier_key_2}, dummy_input)\nparams = flax.core.unfreeze(params)\n\nparams['params']['backbone'] = backbone_params['params']\nparams['batch_stats']['backbone'] = backbone_params['batch_stats']\n\nparams = jax.tree_map(lambda x : x.astype(jnp.bfloat16), params)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:48.526431Z","iopub.execute_input":"2023-03-14T17:56:48.526701Z","iopub.status.idle":"2023-03-14T17:56:51.468010Z","shell.execute_reply.started":"2023-03-14T17:56:48.526676Z","shell.execute_reply":"2023-03-14T17:56:51.467014Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"scheduler = optax.constant_schedule(config.learning_rate)\noptimizer = optax.chain(\n    optax.clip(1.0),\n    optax.adamw(\n        learning_rate=scheduler,\n        weight_decay=config.weight_decay\n    )\n)\n\ntrain_state = {\n    'model': params,\n    'op': optimizer.init(params['params'])\n}\ntrain_state = flax.jax_utils.replicate(train_state)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:51.469133Z","iopub.execute_input":"2023-03-14T17:56:51.469425Z","iopub.status.idle":"2023-03-14T17:56:54.612321Z","shell.execute_reply.started":"2023-03-14T17:56:51.469397Z","shell.execute_reply":"2023-03-14T17:56:54.611150Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def get_accuracy(logits, labels):\n    return (logits.argmax(-1) == labels.argmax(-1)).astype(jnp.float32).mean()\n\n\ndef get_cross_entropy(logits, labels):\n    return -(jax.nn.log_softmax(logits * 16) * labels).sum(-1).mean()","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:54.613666Z","iopub.execute_input":"2023-03-14T17:56:54.613984Z","iopub.status.idle":"2023-03-14T17:56:54.620625Z","shell.execute_reply.started":"2023-03-14T17:56:54.613952Z","shell.execute_reply":"2023-03-14T17:56:54.619746Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def train_step(apply_fn, update_fn, train_state, batch, key):\n    \n    def compute_loss(params):\n        logits, mutable_states = apply_fn(\n            params,\n            batch['image'],\n            mutable='batch_stats',\n            rngs={'dropout': key}\n        )\n        \n        loss = get_cross_entropy(logits, batch['label'])\n        loss = lax.pmean(loss, 'devices')\n        \n        params['batch_stats'] = mutable_states['batch_stats']\n        params['batch_stats'] = jax.tree_map(\n            partial(lax.pmean, axis_name='devices'),\n            params['batch_stats']\n        )\n        \n        return loss, (logits, params)\n    \n    gradient_fn = jax.value_and_grad(compute_loss, has_aux=True)\n    (loss, (logits, train_state['model'])), gradients = gradient_fn(train_state['model'])\n    gradients = lax.pmean(gradients['params'], 'devices')\n    \n    updates, train_state['op'] = update_fn(gradients, train_state['op'], train_state['model']['params'])\n    train_state['model']['params'] = optax.apply_updates(train_state['model']['params'], updates)\n    \n    accuracy = get_accuracy(logits, batch['label'])\n    accuracy = lax.pmean(accuracy, 'devices')\n    \n    return {'train_state': train_state, 'loss': loss, 'accuracy': accuracy}\n\n\nparallelized_train_step = jax.pmap(\n    partial(\n        train_step,\n        apply_fn=classification_model.apply,\n        update_fn=optimizer.update\n    ),\n    axis_name='devices'\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:54.621669Z","iopub.execute_input":"2023-03-14T17:56:54.622127Z","iopub.status.idle":"2023-03-14T17:56:54.634659Z","shell.execute_reply.started":"2023-03-14T17:56:54.622098Z","shell.execute_reply":"2023-03-14T17:56:54.633773Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def val_step(apply_fn, params, batch):\n    logits = apply_fn(params, batch['image'], mutable=False)\n    \n    loss = get_cross_entropy(logits, batch['label'])\n    loss = lax.pmean(loss, 'devices')\n    \n    accuracy = get_accuracy(logits, batch['label'])\n    accuracy = lax.pmean(accuracy, 'devices')\n    \n    return {'loss': loss, 'accuracy': accuracy}\n\n\nparallelized_val_step = jax.pmap(\n    partial(val_step, apply_fn=classification_model.apply),\n    axis_name='devices'\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:54.635670Z","iopub.execute_input":"2023-03-14T17:56:54.636014Z","iopub.status.idle":"2023-03-14T17:56:54.645407Z","shell.execute_reply.started":"2023-03-14T17:56:54.635987Z","shell.execute_reply":"2023-03-14T17:56:54.644566Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def define_dataloaders():\n    train_loader = create_dataloader(\n        TRAIN_FILES,\n        is_labeled=True,\n        is_ordered=False,\n        shuffle_buffer_size=4 * config.batch_size,\n        drop_remainder=True\n    )\n    val_loader = create_dataloader(\n        VAL_FILES,\n        is_labeled=True,\n        is_ordered=True,\n        shuffle_buffer_size=4 * config.batch_size,\n        drop_remainder=True\n    )\n    return train_loader, val_loader","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:54.646452Z","iopub.execute_input":"2023-03-14T17:56:54.646791Z","iopub.status.idle":"2023-03-14T17:56:54.658753Z","shell.execute_reply.started":"2023-03-14T17:56:54.646765Z","shell.execute_reply":"2023-03-14T17:56:54.657959Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, config.epochs + 1):\n    train_loader, val_loader = define_dataloaders()\n    train_state['model'] = flax.core.unfreeze(train_state['model'])\n    train_epoch_loss, train_epoch_accuracy, train_counter = 0, 0, 0\n    val_epoch_loss, val_epoch_accuracy, val_counter = 0, 0, 0\n    \n    # Training\n    train_progress_bar = tqdm(\n        train_loader,\n        total=199,\n        leave=False,\n        desc=f\"Train {epoch}/{config.epochs}\"\n    )\n    for batch in train_progress_bar:\n        random_key, train_key = jax.random.split(random_key)\n        output = parallelized_train_step(\n            train_state=train_state,\n            batch=batch,\n            key=common_utils.shard_prng_key(train_key)\n        )\n        train_state = output['train_state']\n        train_loss = output['loss'][0].item()\n        train_accuracy = output['accuracy'][0].item()\n        train_progress_bar.set_postfix({\n            'epoch': epoch,\n            'train-loss': train_loss,\n            'train-accuracy': train_accuracy\n        })\n        train_epoch_loss += train_loss\n        train_epoch_accuracy += train_accuracy\n        train_counter += 1\n    train_state['model'] = flax.core.freeze(train_state['model'])\n    train_epoch_loss = train_epoch_loss / train_counter\n    train_epoch_accuracy = train_epoch_accuracy / train_counter\n    \n    # Validation\n    val_progress_bar = tqdm(\n        val_loader,\n        total=58,\n        leave=False,\n        desc=f\"Validation {epoch}/{config.epochs}\"\n    )\n    for batch in val_progress_bar:\n        output = parallelized_val_step(params=train_state['model'], batch=batch)\n        val_loss = output['loss'][0].item()\n        val_accuracy = output['accuracy'][0].item()\n        val_progress_bar.set_postfix({\n            'epoch': epoch,\n            'val-loss': val_loss,\n            'val-accuracy': val_accuracy\n        })\n        val_epoch_loss += val_loss\n        val_epoch_accuracy += val_accuracy\n        val_counter += 1\n    val_epoch_loss = val_epoch_loss / train_counter\n    val_epoch_accuracy = val_epoch_accuracy / train_counter\n    \n    wandb.log({\n        \"train-loss\": train_epoch_loss,\n        \"val-loss\": val_epoch_loss,\n        \"train-accuracy\": train_epoch_accuracy,\n        \"val-accuracy\": val_epoch_accuracy,\n    })\n    \n    # Save Checkpoint\n    with open(\"checkpoint.msgpack\", \"wb\") as outfile:\n        state_dict = flax.serialization.to_state_dict(train_state)\n        serialized_state_dict = flax.serialization.msgpack_serialize(state_dict)\n        outfile.write(serialized_state_dict)\n    \n    # Upload Checkpoint as Weights & Biases Artifacts\n    artifact = wandb.Artifact(f'checkpoint-run-{wandb.run.id}', type='train-state')\n    artifact.add_file(\"checkpoint.msgpack\")\n    wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch-{epoch}\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-14T17:56:54.659798Z","iopub.execute_input":"2023-03-14T17:56:54.660058Z","iopub.status.idle":"2023-03-14T18:20:45.950407Z","shell.execute_reply.started":"2023-03-14T17:56:54.660033Z","shell.execute_reply":"2023-03-14T18:20:45.949283Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"                                                                                                              \r","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-03-14T18:20:45.951690Z","iopub.execute_input":"2023-03-14T18:20:45.951985Z","iopub.status.idle":"2023-03-14T18:21:12.624719Z","shell.execute_reply.started":"2023-03-14T18:20:45.951955Z","shell.execute_reply":"2023-03-14T18:21:12.623727Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train-accuracy</td><td>▁▇███</td></tr><tr><td>train-loss</td><td>█▁▁▁▁</td></tr><tr><td>val-accuracy</td><td>▁▆▇██</td></tr><tr><td>val-loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train-accuracy</td><td>1.0</td></tr><tr><td>train-loss</td><td>0.00176</td></tr><tr><td>val-accuracy</td><td>0.26665</td></tr><tr><td>val-loss</td><td>0.13203</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced <strong style=\"color:#cdcd00\">pear-cobbler-2</strong>: <a href=\"https://wandb.ai/ml-colabs/transfer-learning-jax/runs/edbx0tly\" target=\"_blank\">https://wandb.ai/ml-colabs/transfer-learning-jax/runs/edbx0tly</a><br/>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230314_175548-edbx0tly/logs</code>"},"metadata":{}}]}]}